# .github/workflows/data-pipeline.yml
name: Data Pipeline CI

on:
  push:
    branches:
      - main # Trigger on pushes to the main branch
  pull_request:
    branches:
      - main # Trigger on pull requests to the main branch

jobs:
  build-and-run-pipeline:
    runs-on: ubuntu-latest # Use the latest Ubuntu runner

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4 # Action to checkout your code

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3 # Recommended for modern Docker builds

      - name: Make data directory writable
        # Ensure the 'data' directory (and its contents) is writable by the container user
        # This is crucial for the SQLite database to be created/written by the pipeline
        run: sudo chmod -R 777 data

      - name: Build and Run Data Pipeline with Docker Compose
        # This command will:
        # 1. Build the Docker image defined in your Dockerfile.
        # 2. Start the 'data-pipeline' service defined in your docker-compose.yml.
        # Environment variables (secrets) are passed to the shell where docker compose runs,
        # which then makes them available to the service.
        run: docker compose up --build
        env:
          # These secrets are defined in your GitHub repository settings
          DB_URL: ${{ secrets.DB_URL }}
          DB_TABLE_NAME: ${{ secrets.DB_TABLE_NAME }}

      - name: Verify Database Output
        # This step will run after the 'data-pipeline' service has completed.
        # It assumes the pipeline has generated 'bynd_pipeline.db' in the 'data' directory.
        run: |
          echo "Verifying database file presence and content..."
          if [ -f "data/bynd_pipeline.db" ]; then
            echo "Database file 'data/bynd_pipeline.db' found."
            # Optionally, you can add commands here to query the SQLite DB
            # For example, to check if the table exists and has data:
            # sudo apt-get update && sudo apt-get install -y sqlite3
            # sqlite3 data/bynd_pipeline.db "PRAGMA table_info(${DB_TABLE_NAME});" # Check schema
            # sqlite3 data/bynd_pipeline.db "SELECT COUNT(*) FROM ${DB_TABLE_NAME};" # Check row count
            # Ensure DB_TABLE_NAME is available here, if you want to use it
          else
            echo "Error: Database file 'data/bynd_pipeline.db' not found after pipeline run!"
            exit 1
          fi

      - name: Run Python Tests
        # Run unit/integration tests for your Python code
        # This assumes you have tests in the 'tests/' directory
        run: docker compose run data-pipeline pytest tests/

      - name: Upload Database as Artifact
        # If your pipeline generates a database or other output files you want to keep,
        # you can upload them as workflow artifacts.
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-output-db
          path: data/bynd_pipeline.db # Path to the generated SQLite database
          if-no-files-found: warn # Don't fail the workflow if the file isn't there (just warn)